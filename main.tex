\documentclass[11pt,
			   %10pt, 
               %hyperref={colorlinks},
               aspectratio=169
               ]{beamer}
\usetheme{Singapore}
\usecolortheme[snowy]{owl}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[american]{babel}
\usepackage{graphicx}
\usepackage{hyperref}

\usepackage[natbib=true,style=authoryear,backend=bibtex,useprefix=true]{biblatex}

%\setbeamercolor*{bibliography entry title}{fg=black}
%\setbeamercolor*{bibliography entry location}{fg=black}
%\setbeamercolor*{bibliography entry note}{fg=black}
\definecolor{OwlGreen}{RGB}{75,0,130} % easier to see
\setbeamertemplate{bibliography item}{}
\setbeamerfont{caption}{size=\footnotesize}
\setbeamertemplate{frametitle continuation}{}
\renewcommand*{\bibfont}{\scriptsize}
\addbibresource{bibliography.bib}

\author{Patrick Hall}
\title{The Art and Science of Machine Learning Explanations}
\subtitle{A Discussion with Practical Recommendations}
\logo{\includegraphics[height=8pt]{img/h2o_logo.png}}
\institute{\href{https://www.h2o.ai}{H\textsubscript{2}O.ai}}
\date{Aug. 1 2018}
\subject{Machine Learning Interpretability Guidance for Practitioners}

\begin{document}
	
	\maketitle
	
	\begin{frame}
	
		\frametitle{Contents}
		
		\tableofcontents{}
		
	\end{frame}

%-------------------------------------------------------------------------------
	\section{Front Matter}
%-------------------------------------------------------------------------------
	
		\begin{frame}
		
			\frametitle{Obligatory Front Matter}
			
				\begin{itemize}
					
					\item \textbf{What is interpretation?} ``The ability	to explain or to present in understandable terms to	a human.'' (\cite{been_kim1})
					
					\item \textbf{What is a good explanation?} ``When you can no longer keep asking why.'' (\cite{gilpin2018explaining})
					
					\item \textbf{Why should you care?}
					\begin{itemize}
						\item Understanding of an impactful and quickly expanding set of technologies. 
						\item Addressing accidental or intentional discrimination.
						\item Preventing malicious hacking and adversarial attacks.
						\item Enabling regulatory compliance and increased financial margins.
					\end{itemize}
					
				\end{itemize}
			
		\end{frame}

%-------------------------------------------------------------------------------
	\subsection{Notation}
%-------------------------------------------------------------------------------
	
		\begin{frame}
		
			\frametitle{Notation}
			
				\begin{itemize}
					\item \textbf{Spaces}.  
					\begin{itemize}
						\item The input features come from a set  $\mathcal{X}$ contained in a \textit{P}-dimensional input space\\ (i.e. $\mathcal{X} \subset \mathbb{R}^P)$.  
						\item The output responses come from a set $\mathcal{Y}$ contained in a $C$-dimensional output space (i.e. $\mathcal{Y} \subset \mathbb{R}^C$).
					\end{itemize}	
					\bigskip	
					\item \textbf{Dataset}. A dataset $\mathbf{D}$ consists of $N$ tuples of observations:\\ $[(\mathbf{x}^{(0)},\mathbf{y}^{(0)}), (\mathbf{x}^{(1)},\mathbf{y}^{(1)}), \dots, (\mathbf{x}^{(N-1)},\mathbf{y}^{(N-1)})], \mathbf{x}^{(i)} \in \mathcal{X}, \mathbf{y}^{(i)} \in \mathcal{Y}$.\\
					\begin{itemize}
						\item The input data $\mathbf{X}$ is composed of the set of row vectors $\mathbf{x}^{(i)}$. 
						\begin{itemize}
							\item let $\mathcal{P}$ be the set of features  $\{X_0, X_1, \dots, X_{P-1}\}$, where $X_j = \left[x_{j}^{(0)}, x_{j}^{(1)}, \dots, x_{j}^{(N-1)} \right]^T$.
							\item then each $i$-th observation denoted as $\mathbf{x}^{(i)} = \left[x_0^{(i)}, x_1^{(i)}, \dots, x_{P-1}^{(i)} \right]$ is an instance of $\mathcal{P}$.
						\end{itemize}
				\end{itemize}
			\end{itemize}
		
		\end{frame}

%-------------------------------------------------------------------------------
    \subsection{Learning Problem}
%-------------------------------------------------------------------------------
	
		\begin{frame}
		
			\frametitle{Proposed Updates to the Learning Problem}
			
			\begin{figure}[htb]
				\begin{center}
					\includegraphics[height=140pt]{img/learning_problem.png}
					\label{fig:learning_problem}
				\end{center}
			\end{figure}
			
			Adapted from \textbf{Learning From Data} (\cite{lfd}).
			
		\end{frame}

%-------------------------------------------------------------------------------
	\section{Surrogate DT}
%-------------------------------------------------------------------------------

		\begin{frame}[t]
		
			\frametitle{Surrogate Decision Trees (DT)}
			
			\begin{figure}[htb]
				\begin{center}
					\includegraphics[height=90pt]{img/dt_surrogate.png}
					\label{fig:dt_surrogate}
					\caption{$h_{\text{tree}}$ for Taiwanese credit card data \cite{uci}, and for machine-learned GBM response function $g$.}
				\end{center}
			\end{figure}
			
			\vspace{-20 pt}
			
			\begin{itemize}
				
				\item Given a learned function $g$ and set of predictions $g(\mathbf{X})$, a surrogate DT can be trained: $ \mathbf{X}, g(\mathbf{X}) \xrightarrow{\mathcal{A}_{\text{surrogate}}} h_{\text{tree}}$.
		
				\item $h_{\text{tree}}$ displays a low-fidelity, high-interpretability flow chart of $g$'s decision making process, and important features and interactions in $g$.	
			
			\end{itemize}
			
		\end{frame}
	
		\begin{frame}
		
			\frametitle{Surrogate Decision Trees (DT)}
			
			\begin{itemize}
				
				\item Always use error measures to assess the trustworthiness of $h_{\text{tree}}$.
	
				\item Prescribed methods (\cite{dt_surrogate1}; \cite{dt_surrogate2}) for training $h_{\text{tree}}$ do exist. In practice, straightforward cross-validation approaches are typically sufficient. 
				
				\item Comparing cross-validated training error to traditional training error can give an indication of the stability of the single tree model, $h_{\text{tree}}$.
				
				\item Hu et al. (\citeyear{lime-sup}) use local linear surrogate models, $h_{\text{GLM}}$, in $h_{\text{tree}}$ leaf nodes to increase overall surrogate model fidelity while also retaining a high degree of interpretability.
				
			\end{itemize}
			
		\end{frame}

%-------------------------------------------------------------------------------
	\section{PD and ICE}
%-------------------------------------------------------------------------------

		\begin{frame}
		
			\frametitle{Partial Dependence (PD) and Individual Conditional Expectation (ICE)}
			
				\vspace{-10 pt}
			
				\begin{itemize}
					
					\item Following Friedman, Hastie, and Tibshirani (\citeyear{esl}) a single feature $X_j \in \mathbf{X}$ and its complement set $X_{(-j)} \in \mathbf{X}$ (where $X_j \cup X_{(-j)} = \mathbf{X}$) is considered.
					
					\item $\text{PD}(X_j, g)$ for a given feature $X_j$ is estimated as the average output of the learned function $g$ when all the components of $X_j$ are set to a constant $x \in \mathcal{X}$ and $X_{(-j)}$ is left untouched.
					
					\item $\text{ICE}(X_j, \mathbf{x}^{(i)}, g)$ for a given observation $\mathbf{x}^{(i)}$ and feature $X_j$ is estimated as the output of the learned function $g$ when $x^{(i)}_j$ is set to a constant $x \in \mathcal{X}$ and $\mathbf{x}^{(i)} \in X_{(-j)}$ are left untouched.
					
					\item PD and ICE curves are usually plotted over some set of interesting constants $x \in \mathcal{X}$. 
				
				\end{itemize}
			
		\end{frame}
	
		\begin{frame}[t]
		
			\frametitle{Partial Dependence (PD) and Individual Conditional Expectation (ICE)}
		
			\begin{figure}[htb]
				\begin{center}
					\includegraphics[height=100pt]{img/pdp_ice.png}
					\label{fig:pdp_ice}
					\caption{PD and ICE curves for $ X_j = \text{num}_9$, for known signal generating function $f(\mathbf{X}) = \text{num} _1 * \text{num}_4 + |\text{num}_8| * \text{num}_9^2 + e$, and for machine-learned GBM response function $g$.}
				\end{center}
			\end{figure}
			
			\vspace{-10pt}
			
			Overlaying PD and ICE curves is a succinct method for describing global and local prediction behavior and can be used to detect interactions. (\cite{ice_plots}) 
			
		\end{frame}
	
		\begin{frame}[t]
	
			\frametitle{Partial Dependence (PD) and Individual Conditional Expectation (ICE)}
			
			\vspace{-20pt}
				
				\begin{figure}[htb]
					\begin{center}
						\label{fig:dt_surrogate_pdp_ice}
						\includegraphics[height=95pt]{img/dt_surrogate2_pdp_ice2.png}
						\caption{Surrogate DT, PD, and ICE curves for $ X_j = \text{num}_9$, for known signal generating function $f(\mathbf{X}) = \text{num} _1 * \text{num}_4 + |\text{num}_8| * \text{num}_9^2 + e$, and for machine-learned GBM response function $g$.}
					\end{center}
				\end{figure}
			
				Combining Surrogate DT models with PD and ICE curves is a convenient method for detecting, confirming, and understanding important interactions. 
	
		\end{frame}

%-------------------------------------------------------------------------------
	\section{LIME}
%-------------------------------------------------------------------------------

		\begin{frame}
		
			\frametitle{Local Interpretable Model-agnostic Explanations (LIME)}
			
				Ribeiro, Singh, and Guestrin (\citeyear{lime}) define LIME for some observation $\mathbf{x} \in \mathcal{X}$:
			
				\begin{equation*}
					\begin{aligned}
						\underset{h \in \mathcal{H}}{\arg\min}\:\mathcal{L}(g, h, \pi_{\mathbf{X}}) + \Omega(h)
					\end{aligned}
				\end{equation*}
			
				Here $g$ is the function to be explained, $h$ is an interpretable surrogate model of $g$, often a linear model $h_{GLM}$, $\pi_{\mathbf{X}}$ is a weighting function over the domain of $g$, and $\Omega(h)$ limits the complexity of $h$.
			
				\vspace{5pt}
			
				Typically, $h_{GLM}$ is constructed such that $\mathbf{X}^{(*)}, g({X}^{(*)}) \xrightarrow{\mathcal{A}_{\text{surrogate}}} h_{\text{GLM}}$, where $\mathbf{X}^{(*)}$ is a generated sample, $\pi_{\mathbf{X}}$ weighs $\mathbf{X}^{(*)}$ samples by their Euclidean similarity to $\mathbf{x}$, local feature importance is estimated using $\beta_j x_j$, and $L_1$ regularization is used to induce a simplified, sparse $h_{GLM}$. 		
			
			
		\end{frame}
	
		\begin{frame}
		
			\frametitle{Local Interpretable Model-agnostic Explanations (LIME)}
			
			\begin{itemize}
			
				\item LIME is ideal for creating low-fidelity, highly interpretable explanations for non-DT models and for neural network models trained on unstructured data, e.g. deep learning.
			
				\item Always use regression fit measures to assess the trustworthiness of LIMEs.
			
				\item LIME can be difficult to deploy, but there are highly deployable variants. (\cite{lime-sup}; \cite{h2o_mli_booklet})
			
				\item Local feature importance values are offsets from a local intercept.
				
				\begin{itemize}
					
					\item Note that the intercept in LIME can account for the most important local phenomena.
					
					\item Generated LIME samples can contain large proportions of out-of-range data that can lead to unrealistic intercept values. 
					 
				\end{itemize}
			
			\end{itemize}
			
		\end{frame}
	
		\begin{frame}[t]
		
			\begin{itemize}
		
				\item To increase the fidelity of LIMEs, try LIME on discretized input features and on manually constructed interactions.

				\item Use cross-validation to construct standard deviations or even confidence intervals for local feature importance values.

				\item LIME can fail, particularly in the presence of extreme nonlinearity or high-degree interactions.
		
			\end{itemize}
		
		\end{frame}

%-------------------------------------------------------------------------------
	\section{Tree Shap}
%-------------------------------------------------------------------------------

		\begin{frame}[t]
		
			\frametitle{Tree Shap}
		
			Shapley explanations are a class of additive, consistent local feature importance measures with long-standing theoretical support (\cite{shapley}). For some observation $\mathbf{x} \in \mathcal{X}$, Shapley explanations take the form:
			
			\vspace{-5pt}
			
			\begin{equation*}
				\begin{aligned}
					\phi_0 + \sum_{j=0}^{j=\mathcal{P} - 1} \phi_j \mathbf{x}_j^\prime
				\end{aligned}
			\end{equation*}
			
			Here $\mathbf{x}^\prime \in \{0,1\}^\mathcal{P}$ is a binary representation of $\mathbf{x}$ where 0 indicates missingness. Each $\phi_j$ is the local feature importance value associated with $x_j$.
			
			\begin{itemize}
			
				\item Calculating Shapley values directly is typically infeasible, but they can be estimated in different ways.
			
				\item Tree Shap is a specific implementation of Shapley explanations that leverages DT structures to disaggregrate the contribution of each $x_j$ to $g(\mathbf{x})$ in a DT or DT-based ensemble model. (\cite{tree_shap})
				
			\end{itemize}
			
		\end{frame}
	
		\begin{frame}
		
			\frametitle{Tree Shap}
			
			\begin{itemize}
				
				\item Tree Shap is ideal for high-fidelity explanations of DT-based models, perhaps even in regulated applications.
				
				\item Local feature importance values are offsets from a global intercept.
				
				\item LIME can be constrained to become Shapley explanations, i.e. kernel shap.
				
				\item A similar, popular method known as \textit{treeinterpreter} appears untrustworthy when applied to GBM models. 
				
			\end{itemize}
			
		\end{frame}

% High fidelity

%-------------------------------------------------------------------------------
	\section{Recommendations}
%-------------------------------------------------------------------------------

		\begin{frame}
		
			\frametitle{Closing Recommendations}
			
				\begin{itemize}
					
					\item Monotonically constrained XGBoost, Surrogate DT, PD and ICE plots, and Tree Shap are a direct and open source way to create an interpretable nonlinear model.
					
					\item Global and local explanatory techniques are often necessary to explain a model.
					
					\item Use simpler low-fidelity or sparse explanations to understand more accurate and complex high-fidelity explanations.  
					
					\item Seek consistent results across multiple explanatory techniques. 
					
					\item Methods relying on generated data are sometimes unpalatable to users. They want to understand \textit{their} data.
					
					\item Surrogate models can provide low-fidelity explanations for model mechanisms in original feature spaces if $g$ is defined to include feature extraction or engineering.
					
					\item To increase adoption, production deployment of explanatory methods must be straightforward.
					
				\end{itemize}
			
			
		\end{frame}

%-------------------------------------------------------------------------------
	\section{Software}
%-------------------------------------------------------------------------------

		\begin{frame}

			\frametitle{Software Examples and Resources}
			
			\textbf{Comparison of Explanatory Techniques on Simulated Data:}\\
			\href{https://github.com/h2oai/mli-resources/tree/master/lime_shap_treeint_compare}{https://github.com/h2oai/mli-resources/tree/master/lime\_shap\_treeint\_compare}\\
			
			\vspace{10pt}
		
			\textbf{In-depth Explanatory Technique Examples:}\\
			\href{https://github.com/jphall663/interpretable_machine_learning_with_python}{https://github.com/jphall663/interpretable\_machine\_learning\_with\_python}\\
			
			\vspace{10pt}
			
			\textbf{"Awesome" Machine Learning Interpretability Resource List:}\\
			\href{https://github.com/jphall663/awesome-machine-learning-interpretability}{https://github.com/jphall663/awesome-machine-learning-interpretability}
			
		\end{frame}

%-------------------------------------------------------------------------------
%	References
%-------------------------------------------------------------------------------


	\begin{frame}[t, allowframebreaks]
	
		\frametitle{References}
		
		\printbibliography
		
	\end{frame}

\end{document}