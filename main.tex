\documentclass[11pt, 
               %hyperref={colorlinks},
               aspectratio=169
               ]{beamer}
\usetheme{Singapore}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[american]{babel}
\usepackage{graphicx}
\usepackage{hyperref}

\usepackage[natbib=true,style=authoryear,backend=bibtex,useprefix=true]{biblatex}
\setbeamercolor*{bibliography entry title}{fg=black}
\setbeamercolor*{bibliography entry location}{fg=black}
\setbeamercolor*{bibliography entry note}{fg=black}
\setbeamertemplate{bibliography item}{}
\setbeamerfont{caption}{size=\footnotesize}
\renewcommand*{\bibfont}{\scriptsize}
\addbibresource{bibliography.bib}

\author{Patrick Hall}
\title{Machine Learning Interpretability}
\subtitle{The Good, the Bad, and the Ugly}
\logo{\includegraphics[height=8pt]{img/h2o_logo.png}}
\institute{\href{https://www.h2o.ai}{H\textsubscript{2}O.ai}}
\date{Aug. 1 2018}
\subject{Machine Learning Interpretability Guidance for Practitioners}

\begin{document}
	
	\maketitle
	
	\begin{frame}
	
		\frametitle{Contents}
		
		\tableofcontents{}
		
	\end{frame}

%-------------------------------------------------------------------------------
	\section{Front Matter}
%-------------------------------------------------------------------------------
	
		\begin{frame}
		
			\frametitle{Obligatory Front Matter}
			
				\begin{itemize}
					
					\item \textbf{What is interpretation?} “The	ability	to explain or to present in understandable terms to	a human.” \cite{been_kim1}
					
					\item \textbf{What is a good interpretation?} "When you can no longer keep asking why." \cite{gilpin2018explaining}
					
					\item \textbf{Why should you care?}
					\begin{itemize}
						\item Understanding of an impactful and quickly expanding set of technologies. 
						\item Addressing accidental or intentional discrimination.
						\item Preventing malicious hacking and adversarial attacks.
						\item Enabling regulatory compliance and increased financial margins.
					\end{itemize}
					
				\end{itemize}
			
		\end{frame}

%-------------------------------------------------------------------------------
	\subsection{Notation}
%-------------------------------------------------------------------------------
	
		\begin{frame}
		
			\frametitle{Notation}
			
				\begin{itemize}
					\item \textbf{Spaces}.  
					\begin{itemize}
						\item The input features come from a set  $\mathcal{X}$ contained in a \textit{P}-dimensional input space (i.e. $\mathcal{X} \subset \mathbb{R}^P)$.  
						\item The output responses come from a set $\mathcal{Y}$ contained in a $C$-dimensional output space (i.e. $\mathcal{Y} \subset \mathbb{R}^C$).
					\end{itemize}	
					\bigskip	
					\item \textbf{Dataset}. A dataset $\mathbf{D}$ consists of $N$ tuples of observations:\\ $[(\mathbf{x}^{(0)},\mathbf{y}^{(0)}), (\mathbf{x}^{(1)},\mathbf{y}^{(1)}), \dots, (\mathbf{x}^{(N-1)},\mathbf{y}^{(N-1)})], \mathbf{x}^{(i)} \in \mathcal{X}, \mathbf{y}^{(i)} \in \mathcal{Y}$.\\
					\begin{itemize}
						\item The input data $\mathbf{X}$ is composed of the set of row vectors $\mathbf{x}^{(i)}$. 
						\begin{itemize}
							\item let $\mathcal{P}$ be the set of features  $\{X_0, X_1, \dots, X_{P-1}\}$, where $X_j = \left[x_{j}^{(0)}, x_{j}^{(1)}, \dots, x_{j}^{(N-1)} \right]^T$.
							\item then each $i$-th observation denoted as $\mathbf{x}^{(i)} = \left[x_0^{(i)}, x_1^{(i)}, \dots, x_{P-1}^{(i)} \right]$ is an instance of $\mathcal{P}$.
						\end{itemize}
				\end{itemize}
			\end{itemize}
		
		\end{frame}

%-------------------------------------------------------------------------------
    \subsection{Learning Problem}
%-------------------------------------------------------------------------------
	
		\begin{frame}
		
			\frametitle{Proposed Updates to the Learning Problem}
			
			\begin{figure}[htb]
				\begin{center}
					\includegraphics[height=125pt]{img/learning_problem.png}
					\label{fig:learning_problem}
				\end{center}
			\end{figure}
			
			The learning problem. Adapted from \textit{Learning From Data}, \cite{lfd}.
			
		\end{frame}

%-------------------------------------------------------------------------------
	\section{Surrogate DT}
%-------------------------------------------------------------------------------

		\begin{frame}
		
			\frametitle{Surrogate Decision Trees (DT)}
			
			\begin{figure}[htb]
				\begin{center}
					\includegraphics[height=90pt]{img/dt_surrogate.png}
					\label{fig:dt_surrogate}
					\caption{$h_{\text{tree}}$ for Taiwanese credit card data \cite{uci}, and for machine-learned GBM response function $g(X)$.}
				\end{center}
			\end{figure}
			
			\vspace{-20 pt}
			
			\begin{itemize}
				
				\item Given a learned function $g$ and set of predictions, $g(\mathbf{X}) = \hat{\mathbf{Y}}$, a surrogate DT can be trained: $ \mathbf{X},\hat{\mathbf{Y}} \xrightarrow{\mathcal{A}_{\text{surrogate}}} h_{\text{tree}}$.
		
				\item $h_{\text{tree}}$ displays a low-fidelity flow chart of $g$'s decision making process, important features in $g$, and important interactions in $g$.	
			
			\end{itemize}
			
		\end{frame}
	
		\begin{frame}
		
			\frametitle{Surrogate Decision Trees (DT)}
			
			\vspace{-10 pt}
			
			\begin{itemize}
				
				\item Always use error measures to assess the trustworthiness of $h_{\text{tree}}$.
	
				\item Prescribed methods (\cite{dt_surrogate1}; \cite{dt_surrogate2}) for training $h_{\text{tree}}$ do exist. In practice, straightforward cross-validation approaches are typically sufficient. 
				
				\item Comparing cross-validated error to standard training error can give an indication of the stability of the single tree model, $h_{\text{tree}}$.
				
				\item \cite{lime-sup} use local linear surrogate models, $h_{\text{glm}}$, in $h_{\text{tree}}$ leaf nodes to increase overall surrogate model accuracy while retaining a high degree of interpretability.
	
				\item $h_{\text{tree}}$ can provide low-fidelity explanations for model mechanisms in the original feature space if $g$ is defined to include feature extraction.
				
			\end{itemize}
			
		\end{frame}

%-------------------------------------------------------------------------------
	\section{PD and ICE}
%-------------------------------------------------------------------------------

		\begin{frame}
		
			\frametitle{Partial Dependence (PD) and Individual Conditional Expectation (ICE)}
			
				\vspace{-10 pt}
			
				\begin{itemize}
					
					\item Following \cite{esl} a single feature $X_j \in \mathcal{P}$, a $P$-dimensional feature space,  and its complement set $\mathcal{P}_{(-j)}$ (where $X_j \cup \mathcal{P}_{(-j)} = \mathcal{P}$) is considered.
					
					\item $\text{PD}(X_j, g)$ for a given feature $X_j$ is estimated as the average of the output of the learned function $g$, where all the components of $X_j$ are set to a constant $x^{(i)}_j \in X_j$, and $\mathcal{P}_{(-j)}$ is left untouched.
					
					\item $\text{ICE}(\mathbf{x}^{(i)}_j, g)$ for a given row $\mathbf{x}^{(i)}$ and feature $X_j$ is estimated as the output of the learned function $g$ where $x^{(i)}_j$ is set to a constant $x^{(i)}_j \in X_j$ and $\mathbf{x}^{(i)} \in\mathcal{P}_{(-j)}$ are left untouched.
					
					\item PD and ICE are usually plotted over some set of interesting $x^{(i)}_j \in X_j$. 
				
				\end{itemize}
			
		\end{frame}
	
		\begin{frame}
		
			\vspace{-10pt}
		
			\frametitle{Partial Dependence (PD) and Individual Conditional Expectation (ICE)}
		
			\begin{figure}[htb]
				\begin{center}
					\includegraphics[height=100pt]{img/pdp_ice.png}
					\label{fig:pdp_ice}
					\caption{PD and ICE curves for $ X_j = \text{num}_9$, for known signal generating function $f(X) = \text{num} _1 * \text{num}_4 + |\text{num}_8| * \text{num}_9^2 + e$, and for machine-learned GBM response function $g(X)$.}
				\end{center}
			\end{figure}
			
			\vspace{-10pt}
			
			Overlaying PD and ICE curves is a succinct method for describing global and local prediction behavior and can be used to detect interactions. \cite{ice_plots} 
			
		\end{frame}
	
		\begin{frame}
	
			\frametitle{Partial Dependence (PD) and Individual Conditional Expectation (ICE)}
	
				\vspace{-20pt}
				
				\begin{figure}[htb]
					\begin{center}
						\label{fig:dt_surrogate_pdp_ice}
						\includegraphics[height=95pt]{img/dt_surrogate2_pdp_ice2.png}
						\caption{Surrogate DT, PD, and ICE curves for $ X_j = \text{num}_9$, for known signal generating function $f(X) = \text{num} _1 * \text{num}_4 + |\text{num}_8| * \text{num}_9^2 + e$, and for machine-learned GBM response function $g(X)$.}
					\end{center}
				\end{figure}
			
				Combining Surrogate DT models with PD and ICE curves is a convenient method for detecting, confirming, and understanding important interactions. 
	
		\end{frame}

%-------------------------------------------------------------------------------
	\section{LIME}
%-------------------------------------------------------------------------------

		\begin{frame}
		
			\frametitle{Local Interpretable Model-agnostic Explanations (LIME) - \textit{Description}}
			
		\end{frame}
	
		\begin{frame}
		
			\frametitle{Local Interpretable Model-agnostic Explanations (LIME) - \textit{Recommendations}}
			
		\end{frame}

% Map between original features and predictions
% Low Fidelity, Sparse, i.e. human readable
% Unstructured data
% Variants: K-LIME, LIME-SUP

%-------------------------------------------------------------------------------
	\section{Tree Shap}
%-------------------------------------------------------------------------------

		\begin{frame}
		
			\frametitle{Tree Shap - \textit{Description}}
			
		\end{frame}
	
		\begin{frame}
		
			\frametitle{Tree Shap - \textit{Recommendations}}
			
		\end{frame}

% High fidelity

%-------------------------------------------------------------------------------
	\section{Recommendations}
%-------------------------------------------------------------------------------

		\begin{frame}
		
			\frametitle{Closing Recommendations}
			
				\begin{itemize}
					
					\item Monotonically constrained XGBoost, Surrogate DT, PD and ICE plots, and Tree Shap are a direct and open source way to create an interpretable nonlinear model.
					
					\item Global and local explanatory techniques are often necessary to explain a model.
					
					\item Simpler low-fidelity or sparse explanations should help in understanding more accurate and complex high-fidelity explanations.  
					
					\item Seek consistency in results across multiple explanatory techniques. 
					
					\item Methods that rely on generated data are sometimes unpalatable to users. They want to understand \textit{their} data.
					
					\item Beware of uninterpretable features.
					
					\item Consider production deployment of explanatory techniques.
					
				\end{itemize}
			
			
		\end{frame}

%-------------------------------------------------------------------------------
	\section{Software}
%-------------------------------------------------------------------------------

		\begin{frame}

			\frametitle{Software Examples and Resources}
			
			\vspace{-30pt}
			
			\textbf{Comparison of Explanatory Techniques on Simulated Data:}\\
			\href{https://github.com/h2oai/mli-resources/tree/master/lime_shap_treeint_compare}{https://github.com/h2oai/mli-resources/tree/master/lime\_shap\_treeint\_compare}\\
			
			\vspace{10pt}
		
			\textbf{In-depth Explanatory Technique Examples:}\\
			\href{https://github.com/jphall663/interpretable_machine_learning_with_python}{
			https://github.com/jphall663/interpretable\_machine\_learning\_with\_python}\\
			
			\vspace{10pt}
			
			\textbf{"Awesome" Machine Learning Interpretability Resource List:}\\
			\href{https://github.com/jphall663/awesome-machine-learning-interpretability}{https://github.com/jphall663/awesome-machine-learning-interpretability}
			
			
			
		\end{frame}

%-------------------------------------------------------------------------------
%	References
%-------------------------------------------------------------------------------


	\begin{frame}[allowframebreaks]
	
		\frametitle{References}
		
		\printbibliography
		
	\end{frame}

\end{document}